---
title: "Functional Programming & Parallelisation in Spatial Point Pattern Analysis"
subtitle: "Use casC: Study into Airbnb listings in Singapore"
author: "Clara Chua"
institute: "School of Computing & Information Systems,<br/>Singapore Management University"
date: "02/24/2021 (updated: `r Sys.Date()`)"
output:  
  word_document:
    toc: yes
    number_sections: yes
  html_document:
    number_sections: yes
    theme: united
    css: style.css
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE, echo=TRUE, eval=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, message = FALSE, fig.width=12, fig.height=6)
```

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, include=FALSE}
packages = c('tidyverse', 'knitr', 'rgdal', 'maptools', 'sf','raster','spatstat', 'tmap','tmaptools', 'gridExtra', 'leaflet', 'OpenStreetMap', 'microbenchmark', 'doParallel', 'foreach')
for (p in packages){
if(!require(p, character.only = T)){
install.packages(p)
}
library(p,character.only = T)
}
```

# Introduction
BRIEF intro about the project and what it aims to do [NEED TO WRITE THIS STILL]
BRIEF intro about the SPPA aspect and what this talk aims to do
Lay out the sections of the RMD file.  Skip to Section XX if you don't want to know about the data wrangling required

**My project**
Spatial Point Pattern analysis is the study of the location of events generated by a point process within a bounded area of interest.  A point process is a stochastic process where the locations of some events of interest are observed within a bounded area.  It can be used in ecology and agriculture to study the spatial distribution of different species of plants and crops, or in epidemiology, where the growth of a disease can be studied spatially.  We can determine from Spatial Point Pattern analysis whether events (e.g. cases of disease) are clustered, or whether different plant species compete with each other within a specified region. 
Part of my project used Spatial Point Pattern analysis to discover if Airbnb listings are clustered in Singapore and where they occur.  We use Ripley's K-function to test whether there is clustering or competition between listings.   

**This talk**
This talk focuses on the functional programming and parallelisation methods used, shows users how to parallelise the envelope function from `spatstat` using the `doParallel` and `foreach` packages, and discusses the performance of the different methods.  

**The Data**
InsideAirbnb is an independent, non-commercial site that provides publicly available information about a city's Airbnb listings, scraped at regular intervals.  We use the listings table, which has the coordinates for each listing and the neighbourhoods and regions that they belong to.  The dataset used was for Singapore, compiled on 22 June 2020. 

**Structure of this document**
The first section discusses the K-test and the methodology used in the Spatial Point Pattern Analysis in the project.  The next section discusses functional programming and the functions written for this project, using the pre-loaded list of ppp objects.  We then look at the steps to parallelise the envelope function and review the computation performance for each of the methods.  The data wrangling to get the list of ppp objects used is shown in the Appendix. 

## Second order analysis (K-test)

Second-order analysis looks at the strength and type of interactions between events in the point process (R.S. Bivand et al, 2013).  We use this to look for clustering and competition between listings within the geographic study area.  One of the ways to measure this is via the K-function as defined by Ripley, 1976.  Ripley's K-function measures the number of events found up to a given distance of any particular event.  It can be used to summarize a point pattern, test hypotheses and estimate parameters.  

The K-function is 
$$K(t) = \lambda^{-1} E[number\: of\: extra\:events\:within\:distance\:t\:of\:a\:randomly\:chosen\:event]$$

We test the hypothesis that the point processes exhibit Complete Spatial Randomness (CSR) i.e. it follows a homogeneous Poisson process.  This is given by the equation $K(t) = \pi t^2$.  We use the Ripley edge correction to account for any edge effects.  Edge effects arise because boundaries of a study area are arbitrary and points outside the boundary are not counted.  The estimator is given by:

$$\hat{K}(t) = \hat\lambda^{-1} \sum_{i} \sum_{j \neq i} w(l_{i}, l_{j})^{-1} \frac{I(d_{ij} < t)}{N} $$

where $d_{ij}$ is the distance between the ith and jth points, and I(x) is the indicator function with the value 1 if x is true and 0 otherwise.  The edge correction is given by the weight function w(li, lj).   It has the value of 1 when the circle centered at li and passing through the point Ij (with radius $d_{ij}$) is completely inside the study area.  If part of the circle falls outside the study area, then w(li,lj) is the proportion of the circumference of that circle that falls in the study area.  The effects of edge corrections are more important for large t because large circles are more likely to be outside the study area.  

## Methodology

`spatstat` provides a function `Kest()` to compute an unbiased estimate of K(r) and we can run a series of Monte Carlo simulations using the `envelope()` function to compare it to the theoretical value.  We use this to test for the null hypothesis of CSR.  We would reject the null hypothesis if simulated values fall outside the bounds of the theoretical values.  Values higher than the upper bound of theoretical values would imply clustered processes whilst values lower than the lower bound of theoretical values would imply competitive processes.  `spatstat` also offers an alternative to the `Kest()` function for large pattern of points using a Fast Fourier Transform `Kest.fft()`.  

We explored both ways of obtaining K-test results - using the `Kest()` and the `Kest.fft()` functions.  As it was computationally intensive to run the Monte Carlo simulation for the whole of Singapore, we split the analysis by the 5 different subregions (East, West, Central, North and Northwest).  We then ran the Fast Fourier Transform version `Kest.fft()` in the envelope for the whole of Singapore.  We also compared the results from both versions of the K-tests for the Central region using the same parameters; the Central region was used as it had the most number of data points for all room types.

## Loading saved objects
We load the saved list of ppp objects that were stored in `ppp_store2` which consists of ppp objects split by region, then by room type, including one that covers all the different room types.  Hence we would call all the listings in the east by `ppp_store2$east$all` or only the private rooms in the central region by `ppp_store2$central$private`.  These have been rescaled to 'km' for easier interpretation when plotted.  The data wrangling to get this object can be found in the Appendix.  
```{r, echo=TRUE, eval=TRUE}
load("ppp_store2.RData")
```


# Functional Programming
We use functional programming (FP) to avoid duplication of the same code and reduce errors in our analysis.  From the methodology above, we will be repeating the `envelope()` function over the different subregions, and for different room types.  Another repetitive code that would benefit from functional programming is when we plot the envelope results for both the `Kest()` and the `Kest.fft()` tests.  

We can use for loops or use functionals (functions like lapply() or mapply() that take another function as an argument) to replace for loops.  The benefits of using functionals are:

* Readability: Each functional is tailored for a specific task and users can understand why and how it is being used.  This keeps it modular, and makes it easier to find and debug a function, instead of going through lines of code to fix any issues.  

* Better workflows:  Functionals can help to create better workflows as we can pipe data before or after the map() or apply() function.  

* Computation time: As R is a vectorized language, using functionals such as `*apply()` and `map()` will typically reduce computation time, as it applies the function to a vector, matrix or dataframe.  This is not always the case and it will depend on the structure of the data and/or function run.  

## K-Function tests by subregions

``` {r eval=FALSE}
# Set up empty list objects to save Ktestcsr results in
Ktestcsr1 <- list() # results of for-loop function
Ktestcsr2 <- list() # results of functionals
Ktestcsr3 <- list() # results of parallelisation
```
The above code sets out empty list objects to store the envelope simulations for comparison and plotting later. 

### Using for-loops

``` {r}
# Run envelope function for different room types for each region
# Function removes $all from the list and only runs the envelope if the number of points for that room type is non-zero.

runKregion1 <- function(ppp_region) {
  set.seed(123)
  reg_name = paste(deparse(substitute(ppp_region)) %>% gsub("\\$", "  ", .)  %>% word(., -1))
  temp_ppp = ppp_region[-1]
  for (i in seq_along(temp_ppp)) {
    var_name = names(temp_ppp)[i]
    if (temp_ppp[[i]][["n"]] >0) {
      Ktestcsr1[[reg_name]][[var_name]] <<- envelope(temp_ppp[[i]], Kest, nsim=99, rank=1, global=TRUE)
    }
  }
}
```

The above function uses a for loop to run the K-tests for a region, split by the different room types.  The seed is set at the beginning of the function for reproducibility.  It removes the first element (`$all`) from the region, and runs the `envelope()` function if there are non-zero points for that room type. It then saves the results into another list, split by region and room type for plotting access later.  The `envelope()` function was run using the 'best' correction, with 99 simulations.  

We also need to plot the various envelope simulations for each region and each room type for analysis.  As this is a plotting function and is not computationally intensive, we do not need to parallelise the operation.  The code below shows a for-loop function to plot results of the K-tests in a 2 x 2 grid.  This can be changed depending on the number of sub-plots required. 

``` {r}
# Function to plot K-test CSR envelope of room types for each region using for i function
plotsubtypes1 <- function(ktestlist) {
  n = ceiling(length(ktestlist)/2)
  par(mfrow=c(n,2))
  for (i in seq_along(ktestlist)) {
    plot(ktestlist[[i]], .-r~r, xlab="d", ylab="K(d)-r", main= paste(deparse(substitute(ktestlist)), names(ktestlist[i]), sep="_"))
  }
}
```


### Using purrr
``` {r}
# Run envelope function for different room types for each region using purr
# Function removes $all from the list and only runs the envelope if the number of points for that room type is non-zero.

# helper function to be used in functionals
runenv <- function(x) {
    envelope(x, Kest, nsim=99, rank=1, global=TRUE)
  }

runKregion2 <- function(ppp_region) {
  set.seed(123)
  reg_name = paste(deparse(substitute(ppp_region)) %>% gsub("\\$", "  ", .)  %>% word(., -1))
  # create temporary variable removing 'all' from the region list and any room types that are zero
  temp_ppp = ppp_region %>% discard(., names(ppp_region)=="all") %>% discard(., map(., "n") ==0)
  Ktestcsr2[[reg_name]] <<- map(temp_ppp, runenv)
  }
```

The above code shows the same output with functional programming using functions such as `map()` and `discard()` from the `purrr` package that comes with `tidyverse`.  `purrr` enhances R’s FP toolkit by providing a complete and consistent set of tools for working with functions and vectors.  This provides more flexibility - it allows us to filter the lists in place before performing other functions on it.  This means that it could apply to more general cases of any list of list with a subset as it only removes subtypes that have `$all` in the list.  The seed is set at the beginning of the function for reproducibility.  We create a `runenv()` function with the same specified parameters for the K-test (best correction, 99 simulations).  We then use the `map()` function to run elements of the filtered ppp (`temp_ppp`) through the `runenv()` function. 

We also have a plot function using purrr to produce the same plot in a 2 x 2 grid.  This can be changed depending on the number of sub-plots required.   It creates a nested function of plotting an element in the main function and applies the `plotelement` function to the various elements of the K-test list.  

``` {r}
# Function to plot K-test CSR envelope of room types for each region using mapply
plotsubtypes2 <- function(ktestlist, y) {
  n = ceiling(length(ktestlist)/2)
  par(mfrow=c(n,2))
  plotelement <- function(element, y) {
    plot(element, .-r~r, xlab="d", ylab="K(d)-r", main=y)
  }
  mapply(plotelement, ktestlist, paste(deparse(substitute(ktestlist)), names(ktestlist), sep="_"))
}
```


# Parallelisation

## What is parallelisation and why do we need it?

Performing Spatial Point Pattern Analysis (SPPA) can be computationally intensive for larger data sets, or data with non-uniform observation windows.  It took us 60 hours to run the various Central region K-tests sequentially.  There is a need for parallelisation for spatial analysis of larger datasets but there are no in-built parallelization methods in `spatstat`, and users will need to utilise other packages to parallelise operations.  

Parallelisation is the idea that we split the computation work across multiple cores (or processors) simultaneously (i.e. run in parallel) to reduce the computation time.  E.g. if we run a computation in 100 seconds on 1 processor, we should get the same computation done in (100/n) seconds using n processors.  However, this is generally not seen in practice as there are overheads when running in parallel, such as spinning up the clusters and ensuring that each cluster has the required packages and data required to run the computation.

Parallelisation is also dependent on OS – functions such as `mclapply()`or other mc* functions from the `parallel` package (installed with base R) works for Mac and Linux, but not for Windows due to the use of forks.  We use the `doParallel` and `foreach` packages to run this parallelisation.  The following code is OS agnostic, and can be run on multi-core computers (laptop or desktop). 

Users can also use the `doAzureParallel` package [^1] to run the parallelisation on the cloud using Microsoft Azure within the foreach package.  There are also other options for parallelisation on the cloud with other service providers such as Google Cloud Platform, DataBricks, AWS, which will not be discussed here.  

## Setting up 
Firstly we need to set up clusters for parallelisation. We use `detectCores()` to see how many cores there are in the laptop/desktop that can be used.  Typically we use 1 or 2 less cores than the total number of cores to cater for other systems running in the background (e.g. OS).  We then create clusters via 'sockets' using `makeCluster()` and use `registerDoParallel()` to register the parallel backend with the `foreach` package.  

``` {r}
# Set up clusters 
# Set up number of clusters (6 in this case - 2 less than max no. of cores)
nclus <- detectCores() - 2

# Make the clusters and register doParallel with the number of clusters
cl <- makeCluster(nclus)
registerDoParallel(cl)
```

In our case, the parallelisation is done by splitting the number of simulations between the different cores, then combining the results back together.  We used 99 simulations in our sequential K-test simulations and therefore we will declare the number of simulations (100) to be split evenly between the cores.  

While we have set up our clusters, they are a blank slate and we need to load objects, functions and packages that they need to run the computation.  We only export the minimum required to the clusters as each object will take up memory in each of the clusters / processes.  We use `clusterExport()` to export the data to the clusters and `clusterEvalQ()` to export the required libraries to each of the cores.  

``` {r}
# Set up minimum total number of simulations for parallel function to split evenly between them (may be slightly more than 100)
nsims = 100

# Distribute the necessary R Objects
clusterExport(cl, c('ppp_store2', 'nsims','nclus'))
clusterEvalQ(cl, library(tidyverse, spatstat))
```

## Parallelisation Functions
We now set up the functions that we will be using to parallelise the operations.  

**rep_ppp**
For each ppp object we will need to replicate it n number of times for the n cores that we set up in the cluster (`nclus`). We set simplify to FALSE so that it returns a list of ppp objects to iterate over, otherwise it returns a matrix of owins, and not a list of ppp objects. 

**runKobjpar**
This function is the core of the parallel operations.  It takes the argument of the ppp object, runs the rep_ppp function to replicate it.  Then for each of the replicated ppp object, we run an envelope of simulations using the `spatstat` functions envelope, using the passed function `Kest` into the envelope.  We also set the number of simulations to be equally distributed, rounded up to the nearest number using the `ceiling()` function.  Hence we will get a minimum of 100 simulations but sometimes more depending on the number of simulations and cores used.  In this case we will have 102 (17*6) simulations in total.  We need to ensure that `savefuns` is set to true so that each output is saved as an envelope object, otherwise we will not be able to pool them together later.  

Generally, foreach with %do% is used to execute an R expression repeatedly, and return the results in some data structure or object, which is a list by default.   It takes the form of for(iterator) %do% {this function}.  The parallelisation happens in the foreach package with the %dopar% operator.  

**poolpar**
This is a function to pool the envelopes from the outputs from the parallel job into a single envelope.  `spatstat` does this by extracting and combining the simulated data from each object; spatstat then computes a new envelope from the combined set of simulations.  This is why `savefuns=TRUE` is required in the earlier function, so that each object contains the simulated data used to construct the envelope.   

**runKregionpar**
This function brings the 3 helper functions above together to run the parallel tests for all the room types in a region.  Similar to the other 2 functions, we remove the segments with 'all', and any room types that do not exist in that region.  We then run a foreach along the temporary ppp object created, set a seed for reproducibility across the 6 cores, and run the parallelisation for that room type in that region.  Results are then pooled and saved to the Ktestcsr3 store in the form `Ktestcsr3$region$room_type` similar to the other 2 function outputs.  

``` {r}
# Set up functions to be used in the parallelisation

# Function to replicate the ppp object for each core in use
rep_ppp <- function(pppobj) {
            replicate(nclus, pppobj, simplify=FALSE)
}

# Function to run an envelope simulation for 1 ppp object.  This results in a list of envelopes.  
runKobjpar <- function(ppp_obj) {
  ppp_par <- rep_ppp(ppp_obj)
  foreach(i=1:length(ppp_par)) %dopar% {
  set.seed(123)
  spatstat::envelope(ppp_par[[i]], spatstat::Kest, nsim=ceiling(nsims/nclus), savefuns = TRUE, rank=1, global=TRUE)
  }
}

# Function to pool envelope simulation results into 1 envelope
poolpar <- function(x) {
  do.call(pool.envelope, x)
}

runKregionpar <- function(ppp_region) {
  reg_name = paste(deparse(substitute(ppp_region)) %>% gsub("\\$", "  ", .)  %>% word(., -1))
  # create temporary variable removing 'all' from the region list and any room types that are zero
  temp_ppp = ppp_region %>% discard(., names(ppp_region)=="all") %>% discard(., map(., "n") ==0)
  foreach(i=1:length(temp_ppp)) %do% {
    tempname <- names(temp_ppp)[i]
    envtemp[[tempname]] <- runKobjpar(temp_ppp[[i]])
  }
    Ktestcsr3[[reg_name]] <<- map(envtemp, poolpar)
  }
```

### Running the parallel operations

```{r eval=FALSE}
# Run selected region
runKregionpar(ppp_store2$east)

# Run envelope simulation for all regions in the list
map(ppp_store2, runKregionpar)

# Stop Cluster
stopCluster(cl)
```
The code above shows how we run the envelope simulation for 1 region for all room types using our `runKregionpar()` function.  The next code chunk shows the code to run the envelope simulation for all regions in the list.  Once we are done with all the parallelisation, we stop the clusters to clean up.  

# Comparison of performance of for-loop, functional programming and parallelisation

We need to replicate the functions a few times to get a better idea of how well each of the functions do.  We used the package `microbenchmark`, which is a simple wrapper over the in-built `system.time()` function to benchmark the computation time for the functions.  The function gives us some statistics about the replication time (e.g. minimum, lower quartile, median, mean, upper quartile, maximum time) .  We used 10 repetitions and omitted the central region from the benchmarking as it would take too long to run.  The results were saved to an RData file.  

## Benchmarking for-loop and functionals using purrr

``` {r, eval=FALSE}
ktestsummary <- microbenchmark("forloop_east" = {runKregion1(ppp_store2$east)},
                              "FP_east" = {runKregion2(ppp_store2$east)},
                              "forloop_north" = {runKregion1(ppp_store2$north)},
                              "FP_north" = {runKregion2(ppp_store2$north)},
                              "forloop_northeast" = {runKregion1(ppp_store2$northeast)},
                              "FP_northeast" = {runKregion2(ppp_store2$northeast)},
                              "forloop_west" = {runKregion1(ppp_store2$west)},
                              "FP_west" = {runKregion2(ppp_store2$west)},
                              times=10)

save(ktestsummary, file="ktestsummary.Rdata")
```
We compare the performance of each run of K-test summary using the code above.  

``` {r, echo=FALSE}
load("ktestsummary.Rdata")
ktestsummary
```

The dataframe above shows the benchmarks of the K-tests.  As we can see there are no discernible differences in the time it takes to run either for-loop or the map functionals as the items are not vectorized, and are fairly discrete.  

## Benchmarking parallelisation

``` {r, eval=FALSE}
ktestparsummary <- microbenchmark("par_east" = {runKregionpar(ppp_store2$east)},
                                  "par_northeast" = {runKregionpar(ppp_store2$northeast)},
                                  "par_north" = {runKregionpar(ppp_store2$north)},
                                  "par_west" = {runKregionpar(ppp_store2$west)},
                                  times=10)

save(ktestparsummary, file="ktestparsummary.Rdata")
```

``` {r, echo=FALSE}
load("ktestparsummary.Rdata")
ktestparsummary
```

Here we can see that parallelisation is about 4 times faster than using either the for-loop or functionals.  There are some overheads from parallelising the operations as discussed earlier, therefore the gains are not linear (i.e. not 6 times as we used 6 cores).  The most gains came from running the central region, which took close to 5 times faster than doing it sequentially (12 hours vs 60 hours).  

The table below shows all results.  

``` {r}
combisummary <- rbind(ktestsummary, ktestparsummary)
combisummary
save(combisummary, file="combisummary.RData")
```

## Running Central region

``` {r eval=FALSE}
ptm <- proc.time()
ktestparsummary_cent <- microbenchmark("par_central" = {runKregionpar(ppp_store2$central)}, times=1)
centralreg_time <- proc.time() - ptm
save(ktestparsummary_cent, file="Ktestparsumcentral.RData")
load("combisummary.RData")
```
The code above runs the central region once to get the results added to Ktestcsr3.  

``` {r}
#Stop clusters
stopCluster(cl)
```


``` {r}
load("combisummary.RData")
combisummary
```

The dataframe above shows the benchmarks of the K-tests.  There are no discernible differences in the time it takes to run either for-loop or the functionals as the items are not vectorized, and are fairly discrete.  
Parallelisation is about 4 times faster than using either the for-loop or functionals.  There are some overheads from parallelising the operations, therefore the gains are not linear (i.e. not 6 times as we used 6 cores).  The most gains came from running the central region, which took close to 5 times faster than doing it sequentially (12 hours vs 60 hours).  

We also ran benchmarking for the plotting function, with results shown below.  We compared the performance of plotting functions that was created and ran the test 50 times.  As this is a plotting function we included the Central region envelope simulations in this benchmarking.    

We can see that the for-loop performed better (mean and median) for the central, north, northeast regions, but worse for the east and west regions.  However, the difference is not significant to warrant choosing one over the other method.  In terms of customization, the for-loop is better as it is more flexible; however, the functionals are easier to read and understand.    

``` {r}
load("plotsummary.RData")
plotsummary
```

## Results

### Plotting the graphs run by the different functions

``` {r fig.height = 12, fig.caption: "Comparing graphs run by different functions"}
plotsubtypes1(Ktestcsr1$east)
plotsubtypes1(Ktestcsr2$east)
plotsubtypes1(Ktestcsr3$east)
```

The above shows the plots of the east region run by the different functions.  We see that the sequential envelope function (using the for-loop or functionals) produce the same graphs.  However, with the parallelisation and pooling of the envelopes, there is a slight difference in the theoretical bounds as the envelope is computed from different simulations done in parallel, despite using the same seed to produce it. It could be that we may need more simulations per core to produce something more similar to the sequential envelope.  

### Saving results
We save the results of the Ktestcsrs for use in the main SPPA analysis.  We will use Ktestcsr3 as it has the Central region envelope simulations.  
``` {r eval=FALSE}
# Saving results of K test for each region
save(Ktestcsr1, file="Ktestcsr1.RData")
save(Ktestcsr2, file="Ktestcsr2.RData")
save(Ktestcsr3, file="Ktestcsr3.RData")
```


# K-test Using Fast Fourier Transform
The `Kest.fft()` function is an alternative to analyse large pattern of points.  From the spatstat package documentation, `Kest()` computes the distance between each pair of points analytically and would therefore take a long time in computing large datasets (N-1 points * No. of simulations).  The `Kest.fft()` function discretises the point pattern onto a rectangular raster and applies Fast Fourier Transform (FFT) techniques to estimate the K-function.  In essence, the Fast Fourier Transform algorithm allows the estimates to be computed in parallel, by splitting them pairwise, and aggregating them again.

The result depends on the resolution of the pixel raster and requires the standard deviation of the isotropic Gaussian smoothing kernel that is introduced (sigma).  We use the `bw.diggle()` function on the ppp object as an estimate of sigma and pass it into the `Kest.fft()` function.  

## Loading data
``` {r}
# Load the store for Fast Fourier Transform
load("ppp_store.RData")
```


## Functions set-up
Similarly we will be repeating code across different room types, so the following sets up functions to be used in the Fast Fourier Transform analysis.  

The following code calculates the sigma to pass into the FFT function and runs the Kest.fft function for all the different room types.  As this is computationally faster, we can run more simulations (300).  For reproducibility, we set the seed to 123 as before.  As we see from the time taken, it only takes 308 seconds to compute 300 simulations for all room types across the whole region of Singapore, as compared to hours it would have taken using the `Kest()` function.  We set the function to take 4 arguments - the ppp object it will be testing (x), the name of the new object that the K-test results will be saved under (listname), the list of sigma that was calculated (sigmaz), and the number of simulations (nsimfft)

``` {r}
# Create sigma list using the bw.diggle() function for the various room types, saved into a list
sigma_sg <- map(ppp_store, bw.diggle)
```

``` {r}
# Function to run envelope for FFT function 
runfftenv <- function(x, sigmaz, nsimfft) {
  set.seed(123)
  envelope(x, Kest.fft, sigma = sigmaz, nsim = nsimfft, rank = 1, global=TRUE)
}

# Function to run envelope for all room types in the ppp_store
runKfft <- function(x, listname, sigmaz, nsimfft) {
  csrname <- list()
  for (i in seq_along(x)) {
    sigmasel = sigmaz[[i]]
    type_name = names(x)[i]
    csrname[[type_name]] <- runfftenv(x[[i]], sigmasel, nsimfft)
    assign(listname, csrname, env=.GlobalEnv)
  }
}
```

``` {r}
# Running the FFT for all room types
ptm <- proc.time()
runKfft(ppp_store, "Ktestcsrfft", sigma_sg, 300)
proc.time() - ptm
```

The following function produces a figure that shows a plot of the K-test using FFT on the left, and the same plot zoomed in between 0 - 1km on the right.  

``` {r}
# Function to plot K-test FFT CSR envelope of room types
plotKfft <- function(ktestlist) {
  y = paste(deparse(substitute(ktestlist)))
  par(mfrow=c(1,2))
  plot(ktestlist, .-r~r, xlab="d", ylab="K(d)-r", main=y)
  plot(ktestlist, . - r ~ r, xlab="d", ylab="K(d)-r", xlim=range(0,1), main=paste(y, "zoomed-in"))
}
```

#### Private rooms

We are able to make out a distance (r) where the point patterns exhibit signs of clustering, at around the 0.5km mark.  

```{r, fig.width=12, fig.height=5, fig.cap="K-test using FFT - private rooms"}
plotKfft(Ktestcsrfft$private)
```

#### Comparison of FFT method vs normal K-test
We compare the results from the Fast Fourier Transform versus the normal K-test for the Central neighbourhood to see if there are any significant differences using the 2 methods. Following the same steps, we find the sigma for the different room types in the Central neighbourhood, then run the envelope test for them using the same number of simulations as in the K-test.

``` {r}
# Create sigma
sigma_cen <- ppp_store2$central %>% discard(., names(ppp_store2$central) == "all") %>% map(., bw.diggle)
```

``` {r}
# Run FFT method for Central region, using 99 simulations
system.time((runKfft(ppp_store2$central %>% discard(., names(ppp_store2$central)=="all"), "Ktestcsrfftcen", sigma_cen, 99)))
```

The time taken to run the 99 simulations using FFT is a fraction of the normal K-test (67 seconds vs 12 hours even by parallelisation).  We then plot the `Kest.fft()` and `Kest()` results side by side for each room type to see the difference.  

**Comparing results of FFT and normal Ktest*
```{r, fig.height=5, fig.width=12}
# Function to compare FFT vs normal K-test FFT
plotKfftcompare <- function(ktestlist1, ktestlist2) {
  y = paste(deparse(substitute(ktestlist1)))
  z = paste(deparse(substitute(ktestlist2)))
  par(mfrow=c(1,2))
  plot(ktestlist1, .-r~r, xlab="d", ylab="K(d)-r", xlim=range(0,1), main=paste(y, "zoomed-in"))
  plot(ktestlist2, .-r~r, xlab="d", ylab="K(d)-r", xlim=range(0,1), main=paste(z, "zoomed-in"))
}
```
The code above produces the zoomed-in plots of 2 different graphs to compare the results.  


``` {r, fig.width=12, fig.height=5, fig.cap="Comparison of results of normal and FFT tests for Central region - private rooms and entire homes"}
# Compare plots of FFT and normal K test (Private & Entire homes for Central region)
plotKfftcompare(Ktestcsrfftcen$private, Ktestcsr3$central$private)
plotKfftcompare(Ktestcsrfftcen$entire, Ktestcsr3$central$entire)
```
From the above graphs, we can see that the Fast Fourier Transform provides us with a larger range of values in the Monte Carlo simulation.  Both tests lead us to reject the hypothesis of CSR.  However in the case of the Fast Fourier Transform, we can identify a distance where the listings tend to cluster - 0.2km in the case of both private rooms and entire homes in the Central region.  Similarly for hotel rooms and shared rooms in the graphs below, shared rooms in the Central region tend to cluster at the 0.2km mark, whereas hotel rooms cluster at approximately 0.25km.  

# Appendix: Data Preparation for K-test

## Data Structure
We load the listings table and take a first look at the listings data.  
```{r, echo=TRUE, message=FALSE}
# Loading the Data
listings <- read_csv("data/listings.csv")
```

```{r, echo=TRUE, message=FALSE}
# Reviewing the listings data
glimpse(listings)
```
This table provides basic information about the 7,323 listings that were available as at the date compiled (22 June 2020).  We can see the unique listing id, the name of the listing, host name and id, the neighbourhood it is in together with the coordinates of the listing.  Note however that Airbnb randomizes the listing by about 150m so there may be a slight variation in the actual coordinates and the stated ones in the table.  It also provides information about the room type, price of the listing, the minimum nights and some review statistics for the listing (number of reviews, when the last review was, and the number of reviews per month).  There is also some information about how many listings the host has in total, and how many days the listing is available for within a year (availability_365).  

## Geospatial Data Wrangling

To be able to map listings we need to convert the listings data into an sf object.  The `st_as_sf()` function converts any foreign object into an sf object and specifies the coordinates (taken from the longitude and latitude columns in the listings dataframe).  As the long/lat coordinates are based on the WSG84 projection, we assign that to the listings data, and further transform it into SVY21 coordinates to match the neighbourhoods polygon datafile so that they are projected onto the same crs.  The `st_as_sf()` function leaves the original dataframe listings untouched. 

```{r, message=FALSE}
# Convert listings to SF dataframe
listings_sf <- listings %>% 
                st_as_sf(coords = c("longitude", "latitude"),
                         crs = 4326) %>%
                st_transform(crs = 3414)
```

`head()` is used to display the first ten records and their details.  It shows the geometry type (`sfc_point`) and we can check that the projected CRS is SVY21 as intended.  From the records, we can see that it is the same dataframe as the listings with a geometry column, consisting of an `sfc_point` object in each row, that has replaced the longitude and latitude columns in the original listings dataframe.  

```{r}
head(listings_sf)
```

### Handling Spatial Data Outliers
Examining the data from the listings table, we see that there are listings that lie within the Central Water Catchment area, which is a non-residential area (consisting of parks and reservoirs) and we need to investigate them and check that they are attributed correctly.  We see that there are `r nrow(filter(listings, neighbourhood == "Central Water Catchment"))` listings that are in the Central Water Catchment.  Exploring the listings data further we see that some of the names show that they are located wrongly (e.g. 3 mins from Jurong East MRT), which are nowhere near the catchment area.  However, as it is not possible to identify where exactly these listings are located, we will drop these listings from our dataset.  Similarly, there are listings in other neighbourhoods that are not zoned for residential purposes (e.g. Industrial areas, Airports, military / other zones), and we drop these areas.  

``` {r, echo= TRUE, message = FALSE}
# Remove listings in the non-residential areas 
nonreslisting <- c("Central Water Catchment", "Sungei Kadut", "Mandai", "Western Water Catchment", "Tuas", "Pioneer")
listings_clean <- listings_sf %>% filter(!neighbourhood %in% nonreslisting)
```

### Converting objects to SpatialPoints format
We will be using the package `spatstat` for the analysis of the point patterns.  `spatstat` uses ppp objects to store point patterns and owin objects to store observation windows which is the region of interest.  We use the `maptools` package to convert SpatialPoints or SpatialPointsDataFrames to ppp objects and SpatialPolygons to owin objects.  

As some of the other coordinates are in SVY21 format, we will need to convert the cleaned listings dataframe to SVY21 and SpatialPoints format, and add a projection for use in `spatstat`. Once done, we can then convert them into ppp and owin formats for `spatstat` to compute the spatial point patterns and density.    
``` {r, fig.cap="Listings in sp format"}
# listings to be converted from the listings_sf dataframe to SVY21 format (otherwise it will be out of bounds when using Spatstat) and converting to SpatialPoints (sp)
listings_clean <- st_transform(listings_clean, 3414)
listings_sp <- listings_clean %>% dplyr::select(geometry, room_type) %>% as(., Class = "Spatial")
summary(listings_sp)
plot(listings_sp)
```
From the summary above, we can see that the listings data has been transformed to SpatialPoints with the right projections.  

### Wrangling subzone map
Instead of using the neighbourhood map provided from InsideAirbnb, we use the master plan subzone from the Urban Redevelopment Authority of Singapore (URA) as this gives greater details to the various areas.  We also use the `read_osm()`  function from the OpenStreetMap package to load the background map as a raster in tmap's plot mode.  We load this in sf format for mapping purposes; we also assign the crs projection and convert it to a SpatialPolygons object for use in `spatstat`.  

We check to ensure that the geometry is valid and that the crs of the subzone map is attributed correctly.  
``` {r}
# Read in URA's master planning subzones in SF to plot the polygon layer
# Read in OSM raster of listings data for plot view
mpsz_sf <- st_read(dsn = "data/MP14_SUBZONE_WEB_PL.shp", layer="MP14_SUBZONE_WEB_PL")
sg_osm <- read_osm(listings_clean, ext=1.3)

# Ensure that geometry is valid and check crs of subzone map
mpsz_sf <- st_make_valid(mpsz_sf)
all(st_is_valid(mpsz_sf))
crs(mpsz_sf)
```

We use the cleaned data and exclude areas that do not have any listings.  We use the `st_join` function to join the listings data frame that fall within the subzone map, and filter out areas in the subzone map that are empty. 

``` {r fig.cap="Subzone map of Singapore with listings"}
mpsz_sf2 <- mpsz_sf %>% dplyr::select(SUBZONE_N, REGION_N)
mpsz_sf2 <- st_transform(mpsz_sf2, st_crs(listings_clean))
listings_sf2 <- listings_clean %>% dplyr::select(neighbourhood, room_type)
listings_subzones <- st_join(listings_sf2, mpsz_sf2, prepared=TRUE, join=st_within)
subzone_list <- unique(listings_subzones$SUBZONE_N)
mpsz_sf3 <- mpsz_sf %>% filter(SUBZONE_N %in% subzone_list)
plot(mpsz_sf3["SUBZONE_N"])
```

### Splitting data and map into sub-regions

As we have a relatively large sets of data points to analyse, we split the map of Singapore into the 5 sub-regions to make it computationally faster for the second order analysis.   

The following code splits the subzone map into a list of sub-regions that we can call up.  
``` {r, fig.cap="Subzone map by region"}
# Create list of regions to iterate over
region_list <- unique(listings_subzones$REGION_N)

# Create a store of region SF
region_store_sf <- list()

# Create list of sub-regions
for (i in seq_along(region_list)) {
  region_name <- paste(tolower(word(region_list[i],1)))
  region_name <- gsub("-", "", region_name)
  var_name <- paste("sf", region_name, sep="_")
  region_store_sf[[region_name]] <- mpsz_sf3[mpsz_sf3$REGION_N == region_list[i],c("SUBZONE_N", "PLN_AREA_N")]
}
```

```{r}
# Test region_store
plot(region_store_sf$northeast)
```

We then convert the subregion maps into a list of owins for use in the analysis.  We also dissolve all subzone boundaries in the subregion using the `st_union()` function to make the computation of the analysis quicker.  

```{r fig.height = 6, fig.cap="owin with subzone boundaries dissolved"}
# Convert to owin store, with subzone boundaries dissolved
region_store_owin <- list()

for (i in seq_along(region_list)) {
  region_name <- paste(tolower(word(region_list[i],1))) %>% gsub("-", "", .)
  var_name <- paste("sf", region_name, sep="_")
  region_store_owin[[region_name]] <- mpsz_sf3[mpsz_sf3$REGION_N == region_list[i],3] %>% st_union(.) %>% as(., Class = "Spatial") %>% as(., "owin")
}
```

``` {r}
# Plot owin to check
plot(region_store_owin$northeast)
```

Similarly we create a ppp object of the listings using the `as.ppp()` function from spatstat and add marks by room type.  Next, we create a jittered ppp to ensure that there is no overlap of the points.   

``` {r}
# create ppp in owin objects 
listings_ppp <- as.ppp(listings_sp)
marks(listings_ppp) <- factor(listings_clean$room_type)
listing_ppp_jit <- rjitter(listings_ppp, retry = TRUE, nsim = 1, drop = TRUE)
```

### Creating ppp objects

We create 2 lists of ppp objects. The first splits all the marks by sub-regions, while the second is split by sub-regions, then by room type in the form of `ppp_store$region$room_type`.  Hence we would call all the listings in the east by `ppp_store2$east$all` or only the private rooms in the central region by `ppp_store2$central$private`.  These are also rescaled to 'km' for easier interpretation when graphed.  We then save the ppp store into an RData file for use in running the tests in a separate file.

``` {r}
# Create function to map any ppp into owin, re-scaled from m to km
pppinowin <- function(x, y) {
  x[y] %>% rescale(., 1000,"km")
}

# Create ppp_store, to store ppp-in-owin objects split by subregions for further use / analysis
ppp_store1 <- list()

for (i in seq_along(region_list)) {
  region_name <- paste(tolower(word(region_list[i],1))) %>% gsub("-", "", .)
  ppp_store1[[region_name]] <- pppinowin(listing_ppp_jit, region_store_owin[[region_name]])
}
```
``` {r, results='hide', echo=FALSE}
# Plot one graph to check it has rendered correctly
plot(ppp_store1$east)
```

```{r}
# create object store split by region, then by room type of form ppp_store$region$roomtype
types_room <- unique(listings_sf$room_type)
ppp_store2 <- list()
for (i in seq_along(region_list)) {
  region_name <- paste(tolower(word(region_list[i],1))) %>% gsub("-", "", .)
  ppp_store2[[region_name]] <- list()
  for (j in seq_along(types_room)) {
    room_name <- paste(tolower(word(types_room[j],1)))
    # var_name <- paste("ppp_store1", region_name, sep="$")
    ppp_store2[[region_name]][["all"]] <- ppp_store1[[region_name]]
    ppp_store2[[region_name]][[room_name]] <- ppp_store1[[region_name]] [ppp_store1[[region_name]][["marks"]]==types_room[j]]
  }
}
```
``` {r, results='hide', echo=FALSE}
# Plot a graph to test the object
plot(ppp_store2$central$entire)
```

### Creating ppp objects for K-test using the Fast Fourier Transform 
We set up a list of ppp objects covering all of Singapore, split into the different room types and calculate the sigma for the 4 ppp objects.  

``` {r fig.cap="Store of ppp objects"}
# Create ppp for all of Singapore with only the subzones containing listings and dissolve the boundaries  
sg_owin <- mpsz_sf3 %>% st_union(.) %>% as(., Class="Spatial") %>% as(., "owin")  
ppp_all <- listing_ppp_jit[sg_owin] %>% rescale(., 1000, "km")
plot(split(ppp_all))
````

``` {r}
# Create ppp store of the 4 different room types
types_room <- unique(listings_sf$room_type)
ppp_store <- list()

for (i in seq_along(types_room)) {
  room_name <- paste(tolower(word(types_room[i],1)))
  var_name <- paste("ppp", room_name, sep="_") # Construct the name
  ppp_store[[room_name]] <- ppp_all[ppp_all$marks == types_room[i]]
}
```


``` {r}
# Saving data to run SPPA tests
save(ppp_store2, file = "ppp_store2.RData")
save(ppp_store, file= "ppp_store.RData")
```


[^1]: https://github.com/Azure/doAzureParallel







